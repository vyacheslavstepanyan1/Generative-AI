{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DS 235 | HOMEWORK 5 | VYACHESLAV STEPANYAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5ylLFPDECy-"
      },
      "source": [
        "# Fine-Tuning LLMs with QLoRA\n",
        "\n",
        "This notebook will guide you through the process of performing Supervised Fine Tuning (SFT) on pre-trained LLMs. We are going to be using HuggingFace libraries to load, quantize and train an LLM using QLoRA: A technique for memory-efficient training of very large models.\n",
        "\n",
        "We will load a dataset consisting of instruction-response pairs and train a base model to follow instructions in the dataset. Hopefully, we can achieve a decent model, even with training very low amount of parameters using QLoRA.\n",
        "\n",
        "**Note: It is highly recommended to complete the exercises using the CPU to not waste resources. After completing the exercise, you may shift to GPU and train the model!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAsd8fXKRjLT"
      },
      "source": [
        "# 1. Installing the Dependencies\n",
        "\n",
        "We will need:\n",
        "\n",
        "1. [transformers](https://huggingface.co/docs/transformers/index): for loading and using transformer based pre-trained models\n",
        "2. [peft](https://huggingface.co/docs/peft/index) For parameter efficient training methods (LoRA, QLoRA, etc.)\n",
        "3. [bitsandbytes](https://huggingface.co/docs/bitsandbytes/index): For quantization\n",
        "4. [trl](https://huggingface.co/docs/trl/en/sft_trainer): for training\n",
        "\n",
        "Even though this notebook guides you through the usage of these libraries, you are encouraged to explore their functionality of your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q accelerate peft bitsandbytes transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set device to GPU (CUDA) if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qZiFFQqzf7Z"
      },
      "source": [
        "# Login to HuggingFace\n",
        "\n",
        "If you don't have an account, register and create a token at https://huggingface.co/settings/tokens  \n",
        "Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 and click agree to be able to access the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ol7PTSncXWX_"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57f903785cd54b1fbc5e7b7cfa9735e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "import transformers\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdoOOEnJVUZm"
      },
      "source": [
        "## Model and Dataset\n",
        "\n",
        "Here, we define the dataset and the model to be loaded from HuggingFace. We will fine tune Mistral-7B model, which is one of the best open-source models available to date. It was pre-trained on a large dataset of very good quality. However, without SFT, it is not so good to interact with. We will try to perform SFT on the base Mistral model and see how it ends up.\n",
        "\n",
        "We will use the [DataBricks: Dolly 15K](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset for supervised fine-tuning. It contains instruction-completion samples that cover several categories outlined in InstructGPT paper (Q and A, information extraction, etc.). The goal is to teach pre-trained LLMs to follow instructions and respond in desirable ways.\n",
        "Click on the link to go to the HuggingFace page of the dataset. Explore some samples there to get an idea of what model's responses should look like according to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"databricks/databricks-dolly-15k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Mistral-7B-sft-dolly\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adcC05Atdme1"
      },
      "source": [
        "# 2. Loading and Preparing the Dataset\n",
        "\n",
        "We will now proceed with loading the data and preparing training samples. Our dataset contains various instruction-completion samples. The category of the samples is also available.\n",
        "\n",
        "Additionally, note that some samples have context. The instruction and completion are related to the context. We will use different input-output templates for samples depending whether it has context or not. This will let the model know that we want it to use the context to answer our requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cO6lys2yvoLk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['instruction', 'context', 'response', 'category'],\n",
            "    num_rows: 15011\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6PizsUu9gHsC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'instruction': 'When did Virgin Australia start operating?', 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n",
            "{'instruction': 'Which is a species of fish? Tope or Rope', 'context': '', 'response': 'Tope', 'category': 'classification'}\n"
          ]
        }
      ],
      "source": [
        "print(dataset[0])\n",
        "print(dataset[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwVCHfx_hU_r"
      },
      "source": [
        "## Task 1: Tokenization\n",
        "\n",
        "Write a function to tokenize the text with the model's tokenizer and compute the number of tokens. Make sure not to truncate or pad as we will use the number of tokens to filter out long samples. Return a dict with input ids and length of input ids to be able to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n42Gryo5eqwT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': [28705,\n",
              "  13,\n",
              "  2287,\n",
              "  774,\n",
              "  3133,\n",
              "  3112,\n",
              "  28747,\n",
              "  16104,\n",
              "  528,\n",
              "  264,\n",
              "  1274,\n",
              "  302,\n",
              "  28705,\n",
              "  28770,\n",
              "  1179,\n",
              "  4342,\n",
              "  298,\n",
              "  8356,\n",
              "  4336,\n",
              "  28723,\n",
              "  13,\n",
              "  2287,\n",
              "  774,\n",
              "  12107,\n",
              "  28747,\n",
              "  4003,\n",
              "  349,\n",
              "  264,\n",
              "  1274,\n",
              "  302,\n",
              "  28705,\n",
              "  28770,\n",
              "  1179,\n",
              "  4342,\n",
              "  298,\n",
              "  8356,\n",
              "  4336,\n",
              "  28747,\n",
              "  13,\n",
              "  260,\n",
              "  28740,\n",
              "  28723,\n",
              "  413,\n",
              "  270,\n",
              "  680,\n",
              "  2887,\n",
              "  28725,\n",
              "  4012,\n",
              "  6416,\n",
              "  16244,\n",
              "  14082,\n",
              "  304,\n",
              "  14082,\n",
              "  1486,\n",
              "  297,\n",
              "  28670,\n",
              "  1168,\n",
              "  13,\n",
              "  260,\n",
              "  28750,\n",
              "  28723,\n",
              "  3189,\n",
              "  28742,\n",
              "  28707,\n",
              "  9095,\n",
              "  28725,\n",
              "  1943,\n",
              "  304,\n",
              "  511,\n",
              "  2511,\n",
              "  354,\n",
              "  390,\n",
              "  1043,\n",
              "  390,\n",
              "  368,\n",
              "  541,\n",
              "  13,\n",
              "  260,\n",
              "  28770,\n",
              "  28723,\n",
              "  2985,\n",
              "  655,\n",
              "  28670,\n",
              "  628,\n",
              "  16195,\n",
              "  354,\n",
              "  4210,\n",
              "  24336,\n",
              "  13,\n",
              "  13,\n",
              "  2287,\n",
              "  774,\n",
              "  3133,\n",
              "  3112,\n",
              "  28747,\n",
              "  9595,\n",
              "  349,\n",
              "  264,\n",
              "  7018,\n",
              "  302,\n",
              "  8006,\n",
              "  28804,\n",
              "  13731,\n",
              "  8330,\n",
              "  442,\n",
              "  13731,\n",
              "  598,\n",
              "  283,\n",
              "  13,\n",
              "  2287,\n",
              "  774,\n",
              "  12107,\n",
              "  28747,\n",
              "  13731,\n",
              "  8330,\n",
              "  2,\n",
              "  28705,\n",
              "  13,\n",
              "  260],\n",
              " 'lens': 119}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the model tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, padding_side=\"right\", trust_remote_code=True)\n",
        "eos_token = tokenizer.eos_token # Get the eos token for formatting\n",
        "\n",
        "# By default, Mistral's tokenizer doesn't have a pad token. We will add it to be able to pad short sequences.\n",
        "# We will use the <unk> token as the pad token.\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "\n",
        "def tokenize_fn(text, tokenizer):\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    \n",
        "    # Convert tokens to input ids\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    \n",
        "    # Compute the length of input ids\n",
        "    input_ids_lens = len(input_ids)\n",
        "\n",
        "    return { \"ids\": input_ids, \"lens\": input_ids_lens }\n",
        "\n",
        "tokenize_fn(\n",
        "    \"\"\"\n",
        "    ### Instruction: Give me a list of 3 good ways to gain weight.\n",
        "    ### Response: Here is a list of 3 good ways to gain weight:\n",
        "    1. Eat more food, especially highly processed foods and foods high in sugars\n",
        "    2. Don't exercise, sit and do nothing for as long as you can\n",
        "    3. Drink sugary drinks for extra calories\n",
        "\n",
        "    ### Instruction: Which is a species of fish? Escolar or Escobar\n",
        "    ### Response: Escolar</s>\n",
        "    \"\"\",\n",
        "    tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAu9UM8bisBv"
      },
      "source": [
        "# Task 2: Formatting Instruction-Completion Samples for Model Input\n",
        "\n",
        "To train the model, we need to convert the dataset samples into text which can be used as input to the model. We will use a template for formatting samples from the dataset. The created formatting functions will be provided to the [trainer](https://huggingface.co/docs/trl/en/sft_trainer) which will use it to create inputs and train the model. The templates have special delimiters for the context, instruction, and the response. We will, however, compute the loss only for the completion tokens. This will teach the model to complete the response by understanding context and instruction.\n",
        "\n",
        "Write a function to format the context, instruction, response of a single sample into a single text using the following templates. If there is context, use the template that has context header. Otherwise, use the regular template which has only instruction and response headers.\n",
        "\n",
        "Your result should look something like this:\n",
        "\n",
        "**### Instruction: Some text**  \n",
        "**### Response: The response**\n",
        "\n",
        "Then, write another function to do exactly the same for a several samples and return formatted texts. This function will be used to work with a batch of data.\n",
        "\n",
        "The model will process the input and the context and learn to complete the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ys8PhrUJiqp4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Context: test\n",
            " ### Instruction: test\n",
            " ### Response: test</s>\n",
            "### Instruction: test\n",
            " ### Response: test</s>\n",
            "### Context: Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
            " ### Instruction: When did Virgin Australia start operating?\n",
            " ### Response: Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.</s>\n",
            "\n",
            "### Instruction: Which is a species of fish? Tope or Rope\n",
            " ### Response: Tope</s>\n",
            "\n",
            "### Instruction: Why can camels survive for long without water?\n",
            " ### Response: Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.</s>\n"
          ]
        }
      ],
      "source": [
        "template_no_context =  \"### Instruction: {}\\n ### Response: {}\" + eos_token # Template to use when there is no context\n",
        "template_context =  \"### Context: {}\\n ### Instruction: {}\\n ### Response: {}\" + eos_token # Template to use when there is context\n",
        "\n",
        "def get_single_formatted_input(example):\n",
        "    \"\"\"\n",
        "    example: A dict with the following keys\n",
        "      'instruction': str\n",
        "      'context': str\n",
        "      'response': str\n",
        "      'category': str\n",
        "    \"\"\"\n",
        "\n",
        "    if example['context']:\n",
        "        text = template_context.format(example['context'], example['instruction'], example['response'])\n",
        "    else:\n",
        "        text = template_no_context.format(example['instruction'], example['response'])\n",
        "\n",
        "\n",
        "    return text\n",
        "\n",
        "context_sample = { \"instruction\": \"test\", \"context\": \"test\", \"response\": \"test\"}\n",
        "no_context_sample = { \"instruction\": \"test\", \"context\": \"\", \"response\": \"test\"}\n",
        "\n",
        "context_formatted = get_single_formatted_input(context_sample)\n",
        "no_context_formatted = get_single_formatted_input(no_context_sample)\n",
        "\n",
        "print(context_formatted)\n",
        "print(no_context_formatted)\n",
        "\n",
        "def get_formatted_inputs(examples):\n",
        "    \"\"\"\n",
        "    examples: A dict with the following keys\n",
        "      'instruction': a list of str\n",
        "      'context': a list of str\n",
        "      'response': a list of str\n",
        "      'category': a list of str\n",
        "\n",
        "      examples[\"instruction\"][0] will be the instruction of the 1st sample in the batch\n",
        "    \"\"\"\n",
        "    output_texts = []\n",
        "\n",
        "    for i in range(len(examples['instruction'])):\n",
        "        example = {\n",
        "            'instruction': examples['instruction'][i],\n",
        "            'context': examples['context'][i] if 'context' in examples and len(examples['context']) > i else '',\n",
        "            'response': examples['response'][i]\n",
        "        }\n",
        "        output_texts.append(get_single_formatted_input(example))\n",
        "\n",
        "    ### YOUR CODE GOES HERE\n",
        "\n",
        "\n",
        "    return output_texts\n",
        "\n",
        "print(\"\\n\\n\".join(get_formatted_inputs(dataset[0:3])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbqe9CXHoH1H"
      },
      "source": [
        "## Task 3: Filtering Long Samples\n",
        "\n",
        "Write a function that formats samples from the dataset. Then, tokenize the formatted samples and filter those with number of tokens larger than 256."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OJXpOgBFuSrc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered dataset has 15011 samples\n"
          ]
        }
      ],
      "source": [
        "def filter_long_samples(sample):\n",
        "  ### YOUR CODE GOES HERE\n",
        "    formatted_samples = []\n",
        "    for i in range(len(dataset['instruction'])):\n",
        "        example = {\n",
        "            'instruction': dataset['instruction'][i],\n",
        "            'context': dataset['context'][i] if 'context' in dataset and len(dataset['context']) > i else '',\n",
        "            'response': dataset['response'][i]\n",
        "        }\n",
        "        formatted_text = get_single_formatted_input(example)\n",
        "        # Tokenize the formatted text\n",
        "        tokens = tokenizer.tokenize(formatted_text)\n",
        "        # Check the number of tokens\n",
        "        if len(tokens) <= 256:\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "            input_ids_len = len(input_ids)\n",
        "            formatted_samples.append({ \"ids\": input_ids, \"lens\": input_ids_len })\n",
        "        \n",
        "        return formatted_samples['lens'] <= 256\n",
        "\n",
        "\n",
        "print(f\"Filtered dataset has {len(dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ7TRECr8InD"
      },
      "source": [
        "Now, lets split the dataset to proceed with training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gHRAwRIY8H2H"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(seed=42)\n",
        "splits = dataset.train_test_split(test_size=0.05)\n",
        "train_dataset, val_dataset = splits[\"train\"], splits[\"test\"]\n",
        "\n",
        "data_module = dict(train_dataset=train_dataset, eval_dataset=val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C99_AlgHSVdh"
      },
      "source": [
        "# 3. Defining Training Parameters\n",
        "\n",
        "There is a lot of parameters and configuration to define.\n",
        "\n",
        "1. We will need to define the base model and dataset.\n",
        "2. Configure LoRA and define which modules it will target.\n",
        "3. Define quantization configuration to be able to load and train huge models on small GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9ix4tBYVSA_"
      },
      "source": [
        "## Task 4: LoRA Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHEOU0KfXCo3"
      },
      "source": [
        "Here we need to define LoRA configuration to be able to fine-tune Mistral LLM.\n",
        "\n",
        "Do some research and define your initial LoRA parameters.\n",
        "\n",
        "You need to define 4 things:\n",
        "1. The rank *r*\n",
        "2. The scaling factor alpha. It determines how the adaptation layer's weights affect the base model's. Higher alpha means the LoRA layers act more strongly on the base model.\n",
        "3. LoRA dropout for regularization\n",
        "4. The list of modules to attach LoRA adapters to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TC3Cg9pvUycw"
      },
      "outputs": [],
      "source": [
        "# LoRA attention dimension\n",
        "lora_r = 32\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 64\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.05\n",
        "\n",
        "# Modules to target with LoRA. Available options: ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj', 'lm_head']\n",
        "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkvxpv2qVMmQ"
      },
      "source": [
        "## Quantization Configuration\n",
        "\n",
        "Here we define the quantization strategy for training. Quantization is necessary to be able to load very large models in GPUs with small VRAM.\n",
        "\n",
        "We are going to load the models in 4 bit precision for QLoRA training. Training a QLoRA demands specifying quantization and computation data types. We are going to store the weights in 4 bit normal float type. But, we will dequantize the weights and perform computations in 16 bit float during forward and backward passes. This allows us to use less VRAM when storing the model, but still use high precision fp16 format when doing computations. Refer to the QLoRA paper for more details on the quantization and computation.\n",
        "\n",
        "[QLoRA Paper](https://arxiv.org/abs/2305.14314)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vcvolUilU062"
      },
      "outputs": [],
      "source": [
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Activate nested quantization (double quantization). This may save extra VRAM\n",
        "use_nested_quant = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp_XY7TzVH5o"
      },
      "source": [
        "## Training Parameters\n",
        "\n",
        "Here we define the training parameters such as the learning rate, the optimizer, number of training steps, etc. Everything is defined for you, but take your time to read about each option and play around with them. There are brief explanations for each of these parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7SsMbzorU8FF"
      },
      "outputs": [],
      "source": [
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100). Bf16 is a better data type. It is available on newer NVIDIA chips. https://stats.stackexchange.com/questions/637988/understanding-the-advantages-of-bf16-vs-fp16-in-mixed-precision-training#:~:text=Brain%20float%20(BF16)%20and%2016,the%20cost%20of%20reduced%20precision.\n",
        "fp16 = True\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training. You can try increasing this\n",
        "per_device_train_batch_size = 1\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 1\n",
        "\n",
        "# Number of update steps to accumulate the gradients for. Effective batch size becomes (Batch size x Grad Acc). Grad acc is a cheap way to increase batch size and avoid high VRAM usage.\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "# Enable gradient checkpointing. Gradient checkpointing saves VRAM by recomputing activations, instead of storing them in memory after forward pass. https://github.com/cybertronai/gradient-checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient norm (gradient clipping). Take care to use small values when the training is not stable and gradient norms peak.\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.0\n",
        "\n",
        "# Optimizer to use. Paged optimizer is used to save VRAM\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 2\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.05\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 100\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 10\n",
        "\n",
        "# Maximum sequence length to use. We will use smaller context to not run out of memory on 16Gb VRAM.\n",
        "max_seq_length = 256\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL8Cn2IBt-MO"
      },
      "source": [
        "# 4. Training\n",
        "\n",
        "With the dataset formatting and configuration taken care of, we can proceed to training. We will launch a Tensorboard instance to monitor the training. It allows us to monitor the loss and the gradient norms live."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GJOvDm1SXpn_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 21584), started 0:04:36 ago. (Use '!kill 21584' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-4989d5288cfb52ca\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-4989d5288cfb52ca\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IizTzEUs9hpz"
      },
      "source": [
        "Next, we set up the training. Here are the steps:\n",
        "\n",
        "1. We create a quantization config instance using bits and bytes and the parameters that we have defined above\n",
        "2. Next, we load the base model\n",
        "3. Then, we call *prepare_model_for_kbit_training(model)* to prepare it for quantized training\n",
        "4. Then, we create a LoRA configuration and define training parameters.\n",
        "5. Finally, we start a training session.\n",
        "\n",
        "One imporant thing to note is that we need to train the model **ONLY** on the completion/response tokens. This will be taken care of by the **DataCollatorForCompletionOnlyLM** data collator.\n",
        "\n",
        "It will take samples of text, tokenizer them, pad them and ignore the loss function for all tokens that are not part of the completion. To set it up, we have to tell it which part of our text is the completion by specifying a response template. It will ignore loss for all tokens coming before that template.\n",
        "\n",
        "For example:\n",
        "```python\n",
        "response_template = \"### Response:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "Will only compute the loss for tokens coming after ### Response: This is exactly what we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VNCfzCveRbz2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5990244124704c8e8db2b10d03f3e0bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Desktop\\MyProjects\\Speech_Recognition_seamless_large_data\\GenAI_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b2a368889ba4b07b6612d4c636f47aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dede5064be648bba81711e0f05e1be7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df858070a9c149efaf65e10c3c87cfc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e875631c0457449ab2f0a5a693b762ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e178d694b1164d48a06c22976756eeeb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68fc9dfc656f43f4804f9b40c43b9790",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/14260 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "435b0b1ca5cd4de08ba5b7f3192a3eb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/751 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Desktop\\MyProjects\\Speech_Recognition_seamless_large_data\\GenAI_venv\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23ca1ac3f6324ee69315ffe3f0a8b620",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Desktop\\MyProjects\\Speech_Recognition_seamless_large_data\\GenAI_venv\\Lib\\site-packages\\trl\\trainer\\utils.py:141: UserWarning: Could not find response key `### Response:` in the following instance: <s> ### Context: The ANT catalog (or TAO catalog) is a classified product catalog by the U.S. National Security Agency (NSA) of which the version written in 2008–2009 was published by German news magazine Der Spiegel in December 2013. Forty-nine catalog pages with pictures, diagrams and descriptions of espionage devices and spying software were published. The items are available to the Tailored Access Operations unit and are mostly targeted at products from US companies such as Apple, Cisco and Dell. The source is believed to be someone different than Edward Snowden, who is largely responsible for the global surveillance disclosures since 2013. Companies whose products could be compromised have denied any collaboration with the NSA in developing these capabilities. In 2014, a project was started to implement the capabilities from the ANT catalog as open-source hardware and software.\n",
            "\n",
            "Background\n",
            "The Tailored Access Operations unit has existed since the late 90s. Its mission is to collect intelligence on foreign targets of the United States by hacking into computers and telecommunication networks.\n",
            "\n",
            "In 2012 This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\Desktop\\MyProjects\\Speech_Recognition_seamless_large_data\\GenAI_venv\\Lib\\site-packages\\trl\\trainer\\utils.py:141: UserWarning: Could not find response key `### Response:` in the following instance: <s> ### Context: The Leaning Tower of Pisa (Italian: torre pendente di Pisa), or simply, the Tower of Pisa (torre di Pisa [ˈtorre di ˈpiːza; ˈpiːsa]), is the campanile, or freestanding bell tower, of Pisa Cathedral. It is known for its nearly four-degree lean, the result of an unstable foundation. The tower is one of three structures in the Pisa's Cathedral Square (Piazza del Duomo), which includes the cathedral and Pisa Baptistry.\n",
            "\n",
            "The height of the tower is 55.86 metres (183 feet 3 inches) from the ground on the low side and 56.67 m (185 ft 11 in) on the high side. The width of the walls at the base is 2.44 m (8 ft 0 in). Its weight is estimated at 14,500 tonnes (16,000 short tons). The tower has 296 or 294 steps; the seventh floor has two fewer steps on the north-f This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\Desktop\\MyProjects\\Speech_Recognition_seamless_large_data\\GenAI_venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "c:\\Users\\User\\Desktop\\MyProjects\\Speech_Recognition_seamless_large_data\\GenAI_venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "c:\\Users\\User\\Desktop\\MyProjects\\Speech_Recognition_seamless_large_data\\GenAI_venv\\Lib\\site-packages\\trl\\trainer\\utils.py:141: UserWarning: Could not find response key `### Response:` in the following instance: <s> ### Context: The FIFA World Cup, often simply called the World Cup, is an international association football competition contested by the senior men's national teams of the members of the Fédération Internationale de Football Association (FIFA), the sport's global governing body. The tournament has been held every four years since the inaugural tournament in 1930, except in 1942 and 1946 when it was not held because of the Second World War. The reigning champions are Argentina, who won their third title at the 2022 tournament.\n",
            "\n",
            "The format involves a qualification phase, which takes place over the preceding three years, to determine which teams qualify for the tournament phase. In the tournament phase, 32 teams compete for the title at venues within the host nation(s) over about a month. The host nation(s) automatically qualify to the group stage of the tournament. The FIFA World Cup has been scheduled to expand to 48 teams for the 2026 tournament.\n",
            "\n",
            "As of the 2022 FIFA World Cup, 22 final tournaments have been held and a total of 80 national teams have competed. The This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 45.9151, 'train_samples_per_second': 0.174, 'train_steps_per_second': 0.044, 'train_loss': 1.0658118724822998, 'epoch': 0.0}\n"
          ]
        }
      ],
      "source": [
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "# Create a Quantization config with the parameters defined in the previous cell.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "# Quantize the model for low precision training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.config.use_cache = False # Saves a lot of memory\n",
        "model.pretraining_fp = 1\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    target_modules=target_modules,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    max_steps=max_steps,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\",\n",
        "    do_eval=False\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning collator. The response template tells the trainer to compute loss only for the tokens coming after it\n",
        "# We do not need to compute loss for the instruction and context tokens. We need the model to be trained only the completion part.\n",
        "response_template = \"### Response:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "# Create the SFT Trainer. We pass in the formatting function that we have defined alongside the model, dataset, collator, and LoRA configuration\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        "    formatting_func=get_formatted_inputs,\n",
        "    data_collator=collator,\n",
        "    **data_module\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeO2R1XWCLq3"
      },
      "source": [
        "# 5. Generating from the Trained Model\n",
        "\n",
        "Now, we will load the base model and apply the trained LoRA adapters over it. Next, we can try prompting it!\n",
        "\n",
        "Note: You may need to restart session to free up some VRAM to be able to load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "w1ozPHcZoiZk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1176"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Free up the memory to be able to reaload the model\n",
        "\n",
        "import gc\n",
        "# del base_model\n",
        "del trainer\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QQn30cRtAZ-P"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8365b7a229b34fc5a01df14596f8e259",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Desktop\\MyProjects\\Speech_Recognition_seamless_large_data\\GenAI_venv\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Reload model and merge it with LoRA weights. Of course we reload it quantized as well.\n",
        "\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.unk_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Hmd6RZ9e8pQz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> ### Instruction: What do you do when the weather outside is cold and it is raining?\n",
            " ### Response: 101 Warm-Up Drills and Skills for Soccer Coaches\n",
            " ### [Book Website](https://www.amazon.com/Warm-Up-Drills-Soccer-Coaches/dp/1589239681)\n",
            "\n",
            "[![AWS: Warm Up Drills and Skills for Soccer Coaches](http://ecx.images-amazon.com/images/I/410XD12N7TL.jpg)](https://www.amazon.com/Warm-Up-Drills-Soccer\n"
          ]
        }
      ],
      "source": [
        "from transformers import GenerationConfig\n",
        "generation_config = GenerationConfig(max_new_tokens=128, top_p=0.9, do_sample=True, repetition_penalty=1)\n",
        "\n",
        "prompt_template = \"### Instruction: {}\\n ### Response: \"\n",
        "prompt = \"What do you do when the weather outside is cold and it is raining?\"\n",
        "input = tokenizer(prompt_template.format(prompt), return_tensors=\"pt\").to(model.device)[\"input_ids\"]\n",
        "\n",
        "print(tokenizer.decode(model.generate(input, generation_config=generation_config, pad_token_id=tokenizer.pad_token_id)[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
