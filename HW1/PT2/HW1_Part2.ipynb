{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1**: Implementing and analyzing Custom Loss Functions in PyTorch"
      ],
      "metadata": {
        "id": "9FWMS19abYkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This task sequence introduces the development of custom loss functions in PyTorch, with a focus on applying theoretical knowledge to practical implementation. As an initial example, the L1 Loss (Mean Absolute Error) function is fully implemented, demonstrating how to extend PyTorch's nn.Module to create custom loss computations. Following this example **implement** additional loss functions, including L2 Loss (Mean Squared Error), Binary Cross-Entropy Loss, and Cross-Entropy Loss for multi-class classification."
      ],
      "metadata": {
        "id": "pkOwunLxfbyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class L1Loss(nn.Module):\n",
        "    \"\"\"\n",
        "    L1 Loss, also known as Mean Absolute Error (MAE).\n",
        "    \"\"\"\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Forward pass for L1 loss using PyTorch operations.\n",
        "\n",
        "        :param y_pred: Predicted values (Tensor).\n",
        "        :param y_true: Ground truth values (Tensor).\n",
        "        :return: Scalar tensor representing the L1 loss.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO:                                                                #\n",
        "        # Implement the forward pass to calculate the L1 loss.                 #\n",
        "        # Use PyTorch tensor operations to compute the mean absolute difference#\n",
        "        # between y_pred and y_true.                                           #\n",
        "        ########################################################################\n",
        "        return torch.mean(torch.abs(y_pred - y_true))\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Define sample predicted values and ground truth values for testing your implementation.\n",
        "    y_pred = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
        "    y_true = torch.tensor([1.5, 2.5, 3.0, 4.5])\n",
        "\n",
        "    # Initialize your custom L2Loss\n",
        "    criterion = L1Loss()\n",
        "\n",
        "    # TODO: Compute the loss using your L2Loss class and print it.\n",
        "    loss = criterion(y_pred, y_true)\n",
        "    print(f\"Loss: {loss}\")\n",
        "\n",
        "    # TODO: Perform a backward pass to compute gradients (optional demonstration of PyTorch's autograd).\n",
        "    loss.backward()\n",
        "    print(f\"Gradients on y_pred: {y_pred.grad}\")"
      ],
      "metadata": {
        "id": "E2CVdpJMOEhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "758bf6e1-43f4-451d-eef2-dcf11f18bc2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.375\n",
            "Gradients on y_pred: tensor([-0.2500, -0.2500,  0.0000, -0.2500])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class L2Loss(nn.Module):\n",
        "    \"\"\"\n",
        "    L2 Loss, also known as Mean Squared Error (MSE).\n",
        "    \"\"\"\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Forward pass for L2 loss using PyTorch operations.\n",
        "        :param y_pred: Predicted values (Tensor).\n",
        "        :param y_true: Ground truth values (Tensor).\n",
        "        :return: Scalar tensor representing the L2 loss.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO:                                                                #\n",
        "        # Implement the forward pass to calculate the L2 loss.                 #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Define sample predicted values and ground truth values for testing your implementation.\n",
        "    # Ensure y_pred and y_true are PyTorch tensors.\n",
        "\n",
        "    # Initialize your custom L1Loss\n",
        "    criterion = L2Loss()\n",
        "\n",
        "    # TODO: Compute the loss using your L2Loss class and print it.\n",
        "\n",
        "    # TODO: Perform a backward pass to compute gradients (optional demonstration of PyTorch's autograd)."
      ],
      "metadata": {
        "id": "9mYQ1IeTYUHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BCELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Binary Cross-Entropy (BCE) Loss implemented for PyTorch.\n",
        "    Note: PyTorch already provides nn.BCELoss, but implementing it manually can be educational.\n",
        "    \"\"\"\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Forward pass for BCE loss using PyTorch operations.\n",
        "\n",
        "        :param y_pred: Predicted probabilities (Tensor) with values in range [0, 1].\n",
        "        :param y_true: Ground truth values (Tensor) with binary values 0 or 1.\n",
        "        :return: Scalar tensor representing the BCE loss.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO:                                                                #\n",
        "        # Implement the BCE loss calculation here.                             #\n",
        "        # Hint: Use PyTorch's torch.clamp to avoid log(0) which is undefined.  #\n",
        "        # Use torch.log for natural logarithm.                                 #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Define sample predicted values and ground truth values for testing your implementation.\n",
        "    # Ensure y_pred and y_true are PyTorch tensors.\n",
        "\n",
        "    # Initialize your custom BCELoss\n",
        "    criterion = BCELoss()\n",
        "\n",
        "    # TODO: Compute the loss using your BCELoss class and print it.\n",
        "\n",
        "    # TODO: Perform a backward pass to compute gradients (optional demonstration of PyTorch's autograd)."
      ],
      "metadata": {
        "id": "qyonhh7xOHNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the Cross-Entropy Loss for multi-class classification in PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(CELoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        Forward pass for Cross-Entropy loss.\n",
        "\n",
        "        :param logits: Logits from the model (Tensor). Shape: [batch_size, num_classes].\n",
        "        :param targets: Ground truth class indices (Tensor). Shape: [batch_size].\n",
        "        :return: Scalar tensor representing the CE loss.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO:                                                                #\n",
        "        # Implement the forward pass to calculate the Cross-Entropy loss.      #\n",
        "        # Hint: Don't use PyTorch's log_softmax and nll_loss functions.   #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Define sample predicted values and ground truth values for testing your implementation.\n",
        "    # Ensure y_pred and y_true are PyTorch tensors.\n",
        "\n",
        "    # Initialize your custom CELoss\n",
        "    criterion = CELoss()\n",
        "\n",
        "    # TODO: Compute the loss using your CELoss class and print it.\n",
        "\n",
        "    # TODO: Perform a backward pass to compute gradients (optional demonstration of PyTorch's autograd)."
      ],
      "metadata": {
        "id": "CagRBaWXag58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 2:** Implementing Custom Activation Functions in PyTorch\n"
      ],
      "metadata": {
        "id": "dttnSzVVdL7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This task involves developing a set of custom activation functions in PyTorch, understanding their roles in neural networks, and how they can be implemented from scratch. Activation functions are crucial for introducing non-linearity into the network, allowing for the learning of complex patterns in the data. You'll start with an example of the ReLU (Rectified Linear Unit) activation function and then proceed to **implement** additional activation functions such as Sigmoid, Tanh, and Softmax, followed by a **comparison** with PyTorch's built-in implementations.\n",
        "\n",
        "**The backward calculation for the Softmax function is not straightforward; hence, you may rely solely on PyTorch's built-in functionality for the backward pass.**"
      ],
      "metadata": {
        "id": "CHCxEDGyv4w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the ReLU activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ReLU, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for ReLU.\n",
        "        :param x: Input tensor.\n",
        "        :return: Output tensor where ReLU(x) = max(0, x).\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the ReLU activation function.                        #\n",
        "        ########################################################################\n",
        "        return torch.maximum(torch.zeros_like(x), x)\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def backward(grad_output):\n",
        "      \"\"\"\n",
        "      Backward pass for custom ReLU.\n",
        "      :param grad_output: Gradient tensor of the output.\n",
        "      :return: Gradient tensor for the input.\n",
        "      \"\"\"\n",
        "      ########################################################################\n",
        "      # TODO: Implement the backward computation for ReLU.                   #\n",
        "      ########################################################################\n",
        "      # Gradient of ReLU is 1 for input > 0; otherwise, it's 0\n",
        "      grad_input = grad_output.clone()\n",
        "      grad_input[grad_input >= 1] = 1\n",
        "      return grad_input\n",
        "      ########################################################################\n",
        "      #                           END OF YOUR CODE                           #\n",
        "      ########################################################################\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a sample input tensor.\n",
        "    x = torch.tensor([-1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
        "\n",
        "    # Initialize the custom ReLU activation function.\n",
        "    custom_relu = ReLU()\n",
        "\n",
        "    # Compute the activation using the custom ReLU class.\n",
        "    activated_x_custom = custom_relu(x)\n",
        "\n",
        "    # Perform a backward pass to compute gradients for the custom implementation.\n",
        "    gradients_custom = ReLU.backward(activated_x_custom)\n",
        "\n",
        "    # Print the outputs and gradients from the custom implementation.\n",
        "    print(\"Custom ReLU output:\", activated_x_custom)\n",
        "    print(\"Custom ReLU gradients:\", gradients_custom)\n",
        "\n",
        "    # Reset gradients to zero before another backward pass\n",
        "    x.grad = None\n",
        "\n",
        "    # Compute the activation using PyTorch's built-in relu function.\n",
        "    activated_x_torch = torch.relu(x)\n",
        "\n",
        "    # Perform a backward pass to compute gradients for PyTorch's implementation.\n",
        "    activated_x_torch.backward(torch.ones_like(x))\n",
        "    gradients_torch = x.grad\n",
        "\n",
        "    # Print the outputs and gradients from PyTorch's implementation.\n",
        "    print(\"PyTorch ReLU output:\", activated_x_torch)\n",
        "    print(\"PyTorch ReLU gradients:\", gradients_torch)"
      ],
      "metadata": {
        "id": "Krp6yaXKddz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcbd3c4-a2ec-420a-db3b-7a72bb4c8d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom ReLU output: tensor([0., 0., 1., 2.], grad_fn=<MaximumBackward0>)\n",
            "Custom ReLU gradients: tensor([0., 0., 1., 1.], grad_fn=<IndexPutBackward0>)\n",
            "PyTorch ReLU output: tensor([0., 0., 1., 2.], grad_fn=<ReluBackward0>)\n",
            "PyTorch ReLU gradients: tensor([0., 0., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Sigmoid, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for Sigmoid.\n",
        "        :param x: Input tensor.\n",
        "        :return: Output tensor where Sigmoid(x) = 1 / (1 + exp(-x)).\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the Sigmoid activation function.                     #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass for custom Sigmoid.\n",
        "       :param grad_output: Gradient tensor of the output.\n",
        "       :return: Gradient tensor for the input.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the backward computation for Sigmoid.                #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Test your Sigmoid implementation.\n",
        "    pass"
      ],
      "metadata": {
        "id": "gJWfdJJKdeoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the Tanh activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Tanh, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for Tanh.\n",
        "        :param x: Input tensor.\n",
        "        :return: Output tensor where Tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)).\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the Tanh activation function.                        #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass for custom Tanh.\n",
        "       :param grad_output: Gradient tensor of the output.\n",
        "       :return: Gradient tensor for the input.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the backward computation for Tanh                    #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Test your Tanh implementation.\n",
        "    pass"
      ],
      "metadata": {
        "id": "RDdQGqu2di62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the Softmax activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Softmax, self).__init__()\n",
        "\n",
        "    def forward(self, x, dim=1):\n",
        "        \"\"\"\n",
        "        Forward pass for Softmax.\n",
        "        :param x: Input tensor.\n",
        "        :param dim: The dimension Softmax would be applied to.\n",
        "        :return: Output tensor after applying Softmax.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the Softmax activation function.                     #\n",
        "        # Hint: Subtract the maximum value in each row for numerical stability #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Test your Softmax implementation.\n",
        "    pass"
      ],
      "metadata": {
        "id": "GHGtfdCC3jw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3**: Deriving and Understanding the Sigmoid Function"
      ],
      "metadata": {
        "id": "MXn80SlN-hpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sigmoid function is a widely used activation function in the field of machine learning, especially in logistic regression and neural networks. It maps any real-valued number into the range between 0 and 1.\n",
        "\n",
        "1. Given the sigmoid function defined as $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, **compute the derivative** $\\frac{d\\sigma(x)}{dx}$ **with respect to $x$**.\n",
        "\n",
        "2. A special property of the sigmoid function is that its derivative can be expressed in terms of the sigmoid function itself. If we denote $y = \\sigma(x)$, **show how the derivative you've computed can be re-written in terms of $y$**, where $y$ is the output of the sigmoid function.\n",
        "\n",
        "   *Hint: Your answer should only depend on $y$.*\n"
      ],
      "metadata": {
        "id": "wpoKWs6o-HcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Your solution here"
      ],
      "metadata": {
        "id": "Uud8GNyXlXhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4**: Connecting Sigmoid and Softmax Functions"
      ],
      "metadata": {
        "id": "MDE6cU-EmcC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sigmoid and softmax functions are foundational to machine learning, particularly in classification tasks. While the sigmoid function is traditionally used for binary classification, the softmax function generalizes this concept to multi-class problems. The sigmoid function can be seen as a special case of the softmax function when the output space consists of two classes.\n",
        "\n",
        "Consider a binary classification problem and the general form of the softmax function for an arbitrary vector $\\mathbf{z} $ with components $\\mathbf{z_i} $ for $\\mathbf( i = 1, \\ldots, K) $ classes. The softmax function is defined as:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
        "$$\n",
        "\n",
        "Your task is to demonstrate that the softmax function simplifies to the sigmoid function in the context of binary classification.\n",
        "\n",
        "1. **Express the Softmax Function for Two Classes:**\n",
        "   Show the softmax function for a two-class system and define the components of the vector $\\mathbf{z} $ as arbitrary logits without specifying any particular values.\n",
        "\n",
        "2. **Derive the Sigmoid Function from Softmax:**\n",
        "   Simplify the expression for the probability of the first class and show how it is equivalent to the sigmoid function for an arbitrary logit.\n",
        "\n",
        "_Hint: Consider the nature of binary classification and how the probabilities must sum to one._\n"
      ],
      "metadata": {
        "id": "b-um4CM3Kxwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Your solution here"
      ],
      "metadata": {
        "id": "B9nGOPLtlNKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 5:** Understanding Logits and Log Odds"
      ],
      "metadata": {
        "id": "mJerf9xSmf2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In logistic regression and neural networks, the concept of logits and log odds play a central role in modeling probabilities.\n",
        "\n",
        "- **Logits:** The logit function is the inverse of the sigmoid function. It takes a probability value and maps it to the entire real number line, which can be interpreted as the log odds.\n",
        "\n",
        "- **Log Odds:** This is the logarithm of the odds ratio. For a probability $p$, the odds are $\\frac{p}{1-p}$, and the log odds, or logits, is the natural logarithm of this odds: $\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$. In logistic regression, we predict log odds with the linear combination of features, and then convert these predictions into probabilities using the sigmoid function.\n",
        "\n",
        "\n",
        "\n",
        "**The Sigmoid Inverse:** The inverse of the sigmoid function, denoted as $(\\sigma^{-1})$, is the logit function. Given the sigmoid function defined as:\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "**Derive its inverse, $(\\sigma^{-1}(y))$,** which takes a probability and gives the corresponding log odds.\n",
        "\n",
        "Hint: To find the inverse, set $y = \\sigma(x)$, and solve for $x$ in terms of $y$. The result will give you the logit function.\n",
        "\n"
      ],
      "metadata": {
        "id": "qMtKt3gAhRCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Your solution here"
      ],
      "metadata": {
        "id": "63pRkBd-lZxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6: Understanding Backpropagation and the Chain Rule"
      ],
      "metadata": {
        "id": "sJn6xXW91zr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Background\n",
        "\n",
        "Backpropagation is an algorithm commonly used for training neural networks. It leverages the chain rule to calculate the gradient of the loss function with respect to each weight in the network. This gradient tells us how much the loss will change for a small change in the weights, and it's used to update the weights to minimize the loss.\n",
        "\n",
        "The chain rule is a fundamental principle in calculus that is used to find the derivative of composite functions. If we have functions nested within each other, the chain rule allows us to take the derivative of the entire expression by multiplying the derivatives of the constituent functions.\n",
        "\n",
        "1. **Chain Rule for Simple Composition**\n",
        "\n",
        "   Given a function composed as $f(u(x))$, where $u$ is a function of $x$, use the chain rule to find the derivative of $f$ with respect to $x$.\n",
        "\n",
        "   **Example Function:**\n",
        "\n",
        "   Let $f(u) = e^u$ and $u(x) = 2x + 3$. Compute $\\frac{df}{dx}$.\n",
        "\n",
        "   **Solution:**\n",
        "\n",
        "   First, find $\\frac{du}{dx}$ where $u(x) = 2x + 3$. The derivative is $\\frac{du}{dx} = 2$.\n",
        "\n",
        "   Then, compute $\\frac{df}{du}$ for $f(u) = e^u$. The derivative is $\\frac{df}{du} = e^u$.\n",
        "\n",
        "   Multiply $\\frac{du}{dx}$ by $\\frac{df}{du}$ to get $\\frac{df}{dx} = 2e^{(2x+3)}$.\n",
        "\n",
        "2. **Chain Rule for Nested Composition**\n",
        "\n",
        "   For a nested function $f(g(u(x)))$, where $g$ is a function of $u(x)$, and $u$ is a function of $x$, apply the chain rule to compute the derivative of $f$ with respect to $x$.\n",
        "\n",
        "   **Example Function:**\n",
        "\n",
        "   Let $f(g) = \\sin(g)$, $g(u) = u^2$, and $u(x) = 3x - 5$. Find $\\frac{df}{dx}$.\n",
        "\n",
        "   **Solution:**\n",
        "\n",
        "   Start by finding $\\frac{du}{dx}$ for $u(x) = 3x - 5$. The derivative is $\\frac{du}{dx} = 3$.\n",
        "\n",
        "   Next, find $\\frac{dg}{du}$ for $g(u) = u^2$. The derivative is $\\frac{dg}{du} = 2u$.\n",
        "\n",
        "   Then, compute $\\frac{df}{dg}$ for $f(g) = \\sin(g)$. The derivative is $\\frac{df}{dg} = \\cos(g)$.\n",
        "\n",
        "   By the chain rule, $\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{du} \\cdot \\frac{du}{dx} = \\cos(u^2) \\cdot 2u \\cdot 3$.\n",
        "\n",
        "   Substituting $u(x)$ into the derivative, we get $\\frac{df}{dx} = \\cos((3x - 5)^2) \\cdot 2(3x - 5) \\cdot 3$.\n"
      ],
      "metadata": {
        "id": "axREnEadzc-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Consider a neural network with a single neuron that takes two inputs $x_1$ and $x_2$, with weights $w_1$ and $w_2$ respectively, and a bias $b$. The output of the neuron is passed through a hyperbolic tangent activation function:\n",
        "\n",
        "$$\n",
        "f(x) = \\tanh(w_1x_1 + w_2x_2 + b)\n",
        "$$\n",
        "\n",
        "The hyperbolic tangent function, $\\tanh(x)$, is defined as:\n",
        "\n",
        "$$\n",
        "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
        "$$\n",
        "\n",
        "**Compute the partial derivatives of the function $f(x)$ with respect to $w_1$.**"
      ],
      "metadata": {
        "id": "5fYZgIlO1VMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Your solution here"
      ],
      "metadata": {
        "id": "t-fAoasw3axl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 7 (Optional):** Implementing Custom Optimizers in PyTorch"
      ],
      "metadata": {
        "id": "PkvTVdq15T0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, you will delve into the mechanics of optimization algorithms in deep learning by creating custom optimizer classes in PyTorch. Optimizers are the engines that power the learning process, updating model weights based on gradients to minimize loss functions. You will start by understanding the foundational principles of the Gradient Descent optimizer. Following this, you will **implement** custom versions of more advanced optimizers such as Stochastic Gradient Descent (SGD), Momentum, and Adam, and **compare** their performance with PyTorch's built-in optimizers."
      ],
      "metadata": {
        "id": "Xqhlw6vm7s6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescentOptimizer:\n",
        "    \"\"\"\n",
        "    Custom implementation of the gradient descent optimization algorithm.\n",
        "    \"\"\"\n",
        "    def __init__(self, parameters, learning_rate):\n",
        "        \"\"\"\n",
        "        Initializes the GradientDescentOptimizer.\n",
        "\n",
        "        Args:\n",
        "            parameters (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n",
        "            learning_rate (float): Learning rate for the optimizer.\n",
        "        \"\"\"\n",
        "        self.parameters = list(parameters)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step using gradient descent.\n",
        "\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the gradient descent update step.                    #\n",
        "        ########################################################################\n",
        "        with torch.no_grad():\n",
        "            for param in self.parameters:\n",
        "                if param.grad is not None:\n",
        "                    param.data -= self.learning_rate * param.grad\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Clears gradients of all optimized parameters.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Clear gradients of all parameters.                             #\n",
        "        ########################################################################\n",
        "        for param in self.parameters:\n",
        "            if param.grad is not None:\n",
        "                param.grad.zero_()\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a simple model and a sample loss function\n",
        "    model = torch.nn.Linear(1, 1)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    # Define sample input and target data\n",
        "    input_data = torch.tensor([[1.0], [2.0]], requires_grad=True)\n",
        "    target_data = torch.tensor([[2.0], [4.0]])\n",
        "\n",
        "    # Initialize the custom optimizer\n",
        "    optimizer = GradientDescentOptimizer(model.parameters(), learning_rate=0.01)\n",
        "\n",
        "    # Forward pass: Compute predicted y by passing input_data to the model\n",
        "    predicted_y = model(input_data)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_fn(predicted_y, target_data)\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Updated model weights: {model.weight.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR6-L2NCUuMS",
        "outputId": "e5ace9ce-555d-4c99-9118-47b3bc168f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated model weights: tensor([[-0.4420]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDOptimizer:\n",
        "    \"\"\"\n",
        "    Custom implementation of the stochastic gradient descent optimization algorithm.\n",
        "\n",
        "    Attributes:\n",
        "        parameters (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, parameters, learning_rate):\n",
        "        \"\"\"\n",
        "        Initializes the SGDOptimizer.\n",
        "\n",
        "        Args:\n",
        "            parameters (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n",
        "            learning_rate (float): Learning rate for the optimizer.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Initialize parameters and learning rate.                       #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Perform a single optimization step using SGD.\n",
        "\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the SGD update step.                                 #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Clear gradients of all optimized parameters.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Clear gradients of all parameters.                             #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Repeat the process above for the SGD optimizer"
      ],
      "metadata": {
        "id": "NEyh5JA7XPGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MomentumOptimizer:\n",
        "    \"\"\"\n",
        "    Custom implementation of the stochastic gradient descent optimization algorithm with momentum.\n",
        "    \"\"\"\n",
        "    def __init__(self, parameters, learning_rate, momentum=0.9):\n",
        "        \"\"\"\n",
        "        Initializes the MomentumOptimizer.\n",
        "\n",
        "        Args:\n",
        "            parameters (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n",
        "            learning_rate (float): Learning rate for the optimizer.\n",
        "            momentum (float): Momentum factor (default: 0.9).\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Initialize parameters, learning rate, and momentum.            #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Perform a single optimization step using SGD with momentum.\n",
        "\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the momentum update step.                            #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Clear gradients of all optimized parameters.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Clear gradients of all parameters.                             #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Repeat the process above for the SGD with momentum optimizer"
      ],
      "metadata": {
        "id": "a79dqDEqZNLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamOptimizer:\n",
        "    \"\"\"\n",
        "    Custom implementation of the Adam optimization algorithm.\n",
        "    \"\"\"\n",
        "    def __init__(self, parameters, learning_rate=0.001, betas=(0.9, 0.999), eps=1e-8):\n",
        "        \"\"\"\n",
        "        Initializes the AdamOptimizer.\n",
        "\n",
        "        Args:\n",
        "            parameters (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n",
        "            learning_rate (float): Learning rate for the optimizer.\n",
        "            betas (Tuple[float, float]): Coefficients used for computing running averages of gradient and its square.\n",
        "            eps (float): Term added to the denominator to improve numerical stability.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Initialize parameters, learning rate, betas, and eps.          #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Perform a single optimization step using Adam.\n",
        "\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Implement the Adam update step.                                #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Clear gradients of all optimized parameters.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        # TODO: Clear gradients of all parameters.                             #\n",
        "        ########################################################################\n",
        "        pass\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: Repeat the process above for the Adam optimizer"
      ],
      "metadata": {
        "id": "Sk77xoapZvja"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}